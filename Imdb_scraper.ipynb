{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Importing Libraries**\n","\n","The first step in the process is to import libraries."],"metadata":{"id":"NF7Y4Vboj5ed"}},{"cell_type":"markdown","source":["Goal: We're going to extract the top 250 movies from imdb using Python beautifulsoup, lxml and a few other libraries."],"metadata":{"id":"9Nb-TujIlUMF"}},{"cell_type":"markdown","source":["First stage is importing the libraries we need to use for the data extraction.At each stage we will be explaining these libraries and their usage[link text]"],"metadata":{"id":"Rr5H3s8cm1dB"}},{"cell_type":"code","source":["!pip install unidecode\n","import requests\n","from bs4 import BeautifulSoup\n","from lxml import etree as et\n","import time\n","import random\n","import json\n","from unidecode import unidecode"],"metadata":{"id":"dhqqil7eljdW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703774891533,"user_tz":-420,"elapsed":5707,"user":{"displayName":"Juan Sabastian","userId":"04274776569199184735"}},"outputId":"f3d1fd9a-5322-4bf9-f89d-ad23d7ea5c0b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.7\n"]}]},{"cell_type":"code","source":["start_url = \"https://www.imdb.com/chart/top\"\n","header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\"}\n","movie_urls = []"],"metadata":{"id":"iy1gG5kJmxyh","executionInfo":{"status":"ok","timestamp":1703774895986,"user_tz":-420,"elapsed":2,"user":{"displayName":"Juan Sabastian","userId":"04274776569199184735"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["The next step is to get the link to the movies in the start url and save them to a list we declared above.First we use the requests library to get the url, then we used beautifulsoup to get a beautifulsoup object. The next step is to create a way to query the html for the link. For that we used the etree module from lxml.Upon inspection using chrome developer tools - we found the data is available at an xpath given in the expression below. The expression generates a list of urls"],"metadata":{"id":"o4whMEUOmy9P"}},{"cell_type":"code","source":["response = requests.get(start_url, headers=header)\n","soup = BeautifulSoup(response.content, 'html.parser')\n","dom = et.HTML(str(soup))\n","movie_urls_list = dom.xpath('//td[@class=\"titleColumn\"]/a/@href')"],"metadata":{"id":"cQedtzrtnjdj","executionInfo":{"status":"ok","timestamp":1703774900933,"user_tz":-420,"elapsed":2455,"user":{"displayName":"Juan Sabastian","userId":"04274776569199184735"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["The data in the movies_urls_list is not in the way we need, for example it does not have the imdb domain name with it, also it is too long.\n","\n","We concatonated imdb url into the url string we obtained. However upon inspection we can see that even if we remove all items after the question mark - it is still a valid link going to the same page.We add this to the movie_urls list."],"metadata":{"id":"dyqm_Cjsolkv"}},{"cell_type":"code","source":["for i in movie_urls_list:\n","    long_url = \"https://www.imdb.com\" + i\n","    short_url = long_url.split(\"?\")[0]\n","    movie_urls.append(short_url)"],"metadata":{"id":"H5rpc9YuoyzA","executionInfo":{"status":"ok","timestamp":1703774903548,"user_tz":-420,"elapsed":2,"user":{"displayName":"Juan Sabastian","userId":"04274776569199184735"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Once we get the movies urls into the list - the next step is to go to each movie page and extract the data. However before doing that we need to fix the attributes and the structure we are going to be using..\n","\n","For education purpose - we will be using the json format to store the data. Before parsing the data - we need to prepare writing the data into the json.\n","\n","See the code below to understand how."],"metadata":{"id":"jl045871pVZA"}},{"cell_type":"code","source":["def time_delay():\n","    time.sleep(random.randint(2, 5))"],"metadata":{"id":"-ExNHKFosRNT","executionInfo":{"status":"ok","timestamp":1703774906388,"user_tz":-420,"elapsed":2,"user":{"displayName":"Juan Sabastian","userId":"04274776569199184735"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["with open(\"data_v1.json\", \"w\") as f:\n","    json.dump([], f)\n","\n","\n","def write_to_json(new_data, filename='data_v1.json'):\n","    with open(filename, 'r+') as file:\n","        file_data = json.load(file)\n","        file_data.append(new_data)\n","        file.seek(0)\n","        json.dump(file_data, file, indent=4)"],"metadata":{"id":"DHPyM0DgqCBF","executionInfo":{"status":"ok","timestamp":1703774908762,"user_tz":-420,"elapsed":543,"user":{"displayName":"Juan Sabastian","userId":"04274776569199184735"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["The next stage is to extract the individual elements from each page and write it into a json file.\n","\n","note a couple of things.\n","\n","1- we used a library unidecode,the function unidecode() takes Unicode data and tries to represent it in ASCII characters.\n","\n","The best way to understand this is to not use it and inspect the data - you'll see some strange letters inbetween text. Using unidecode eliminates that problem.\n","\n","2 - we used write json function to write data into the json file"],"metadata":{"id":"86m6QAv-quR7"}},{"cell_type":"code","source":["for movie_url in movie_urls:\n","    response = requests.get(movie_url, headers=header)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    dom = et.HTML(str(soup))\n","\n","    rank = movie_urls.index(movie_url) + 1\n","    movie_name = dom.xpath('//h1[@data-testid=\"hero-title-block__title\"]/text()')[0]\n","    movie_year = dom.xpath('//a[@class=\"ipc-link ipc-link--baseAlt ipc-link--inherit-color sc-8c396aa2-1 WIUyh\"]/text()')[0]\n","    genre = dom.xpath('//span[@class=\"ipc-chip__text\"]/text()')\n","    director_name = dom.xpath('//a[@class=\"ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link\"]/text()')[0]\n","    rating = dom.xpath('//span[@class=\"sc-7ab21ed2-1 jGRxWM\"]/text()')[0]\n","    actors_list = dom.xpath('//a[@data-testid=\"title-cast-item__actor\"]/text()')\n","    actors_list = [unidecode(i) for i in actors_list]\n","\n","    write_to_json({'rank': rank,\n","                   'movie_name': movie_name,\n","                   'movie_url': movie_url,\n","                   'movie_year': movie_year,\n","                   'genre': genre,\n","                   'director_name': unidecode(director_name),\n","                   'rating': rating,\n","                   'actors': actors_list})\n","\n","    time_delay()\n","    print(\"{} written to json file\".format(rank))"],"metadata":{"id":"a7qMgYaTq5Wb","executionInfo":{"status":"ok","timestamp":1703774911678,"user_tz":-420,"elapsed":329,"user":{"displayName":"Juan Sabastian","userId":"04274776569199184735"}}},"execution_count":8,"outputs":[]}]}